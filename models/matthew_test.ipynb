{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do (development notes only)\n",
    "# REMOVE LATER BEFORE SUBMITTING\n",
    "### Feature Engineering\n",
    "* Other model experimentation / optimization\n",
    "    - (model examples including nonlinear)\n",
    "    - K Nearest Neighbors\n",
    "    - Naive Bayes\n",
    "        * Multinomial\n",
    "        * Gaussian\n",
    "        * Bernoulli\n",
    "    - GMM\n",
    "    - SVM\n",
    "* Discuss pros/cons of each of the models compared to results\n",
    "\n",
    "\n",
    "### Model Selection\n",
    "Test with models that can support predict_proba to predict probability of all categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Final Project\n",
    "### Kaggle Competition\n",
    "[San Francisco Crime Statistics](https://www.kaggle.com/c/sf-crime)\n",
    "  \n",
    "###  Team Members\n",
    "Chuck Bolin, Matthew Burke, Yun-Hui Fan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn import utils\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "Load in training and test data to variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_all = pd.read_csv('../data/train.csv', delimiter=',', parse_dates=['Dates'])\n",
    "# test_final contains data which will be submitted to kaggle after predicting categories\n",
    "# Includes ID field not found in training data\n",
    "# Does not include description, category or resolution fields in training data\n",
    "test_final = pd.read_csv('../data/test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Date Features\n",
    "Take the single time to create features for Year, Month, Day and Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dateAttributes(df):\n",
    "    df = df.copy()\n",
    "    df['Year'] = pd.DatetimeIndex(df['Dates']).year\n",
    "    df['Month'] = pd.DatetimeIndex(df['Dates']).month\n",
    "    df['Day'] = pd.DatetimeIndex(df['Dates']).day\n",
    "    df['Hour'] = pd.DatetimeIndex(df['Dates']).hour\n",
    "    return df\n",
    "\n",
    "# Extract elements of dates\n",
    "train_all = dateAttributes(train_all)\n",
    "\n",
    "# Column names to binarize:\n",
    "bin_cols = ['Year', 'Month', 'Hour', 'DayOfWeek', 'PdDistrict']\n",
    "\n",
    "# Binarize columns identified above and add to dataframe\n",
    "for column in bin_cols:\n",
    "    dummies = pd.get_dummies(train_all[column])\n",
    "    train_all[dummies.columns] = dummies\n",
    "\n",
    "# Encode all categories into integers for use later\n",
    "le = LabelEncoder()\n",
    "labels_all = le.fit_transform(train_all['Category'])\n",
    "\n",
    "    \n",
    "#Extract categories names from encoder for use later\n",
    "categories = le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XY Coordinates\n",
    "Round X and Y features to 2 decimal places, convert to strings and then combine into single value\n",
    "Binarize new feature and concatenate to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fun = lambda x: str(round(x, 2))\n",
    "temp = train_all['X'].map(fun) + train_all['Y'].map(fun)\n",
    "lb = LabelBinarizer()\n",
    "train_all = np.concatenate((train_all, lb.fit_transform(temp)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Addresses\n",
    "Extract street names from `Address` column and combine if multiple street names, i.e. for a city block.\n",
    "Ultimately creates 12000+ features, which isn't necessarily more helpful than XY coordinates, especially since we are removing the block numbers which reduces precision even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Street Name Features: 12758\n"
     ]
    }
   ],
   "source": [
    "add = train_all[:,6]\n",
    "streets = []\n",
    "for i in range(0,len(add)):\n",
    "    matches = re.findall('[A-Z]{3,}|[0-9]+TH',add[i])\n",
    "    if len(matches)==1:\n",
    "        streets.append(matches[0])\n",
    "    else:\n",
    "        streets.append(''.join(matches))\n",
    "print 'Total Street Name Features:', len(np.unique(streets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Train/Test\n",
    "Designate 1/3 of the data to be test data and the other 2/3 to be training data for development purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check that data has been formed correctly:\n",
      "\n",
      "Training data shape:  (588292, 206)\n",
      "Training labels shape:  588292\n",
      "Test data shape:  (289757, 206)\n",
      "Test labels shape:  289757\n",
      "('Train labels: ', 39)\n",
      "('Test  labels: ', 39)\n",
      "\n",
      "Top row of training data:\n",
      "\n",
      "[Timestamp('2006-02-07 07:45:00') 'VEHICLE THEFT' 'STOLEN TRUCK' 'Tuesday'\n",
      " 'BAYVIEW' 'NONE' '1400 Block of WAYLAND ST' -122.41645145514299\n",
      " 37.7230044830156 2006 2 7 7 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Define fraction of data to be used as test data\n",
    "fraction = 0.33\n",
    "\n",
    "# Split data into training and test data/labels randomly according to fraction specified\n",
    "train_labels, test_labels, train_data, test_data = train_test_split(labels_all, train_all, test_size=fraction)\n",
    "\n",
    "\n",
    "print 'Check that data has been formed correctly:\\n'\n",
    "\n",
    "print 'Training data shape: ', train_data.shape\n",
    "print 'Training labels shape: ', len(train_labels)#train_labels.shape\n",
    "\n",
    "print 'Test data shape: ', test_data.shape\n",
    "print 'Test labels shape: ', len(test_labels)#test_labels.shape\n",
    "\n",
    "print('Train labels: ', len(np.unique(train_labels)))\n",
    "print('Test  labels: ', len(np.unique(test_labels)))\n",
    "\n",
    "print '\\nTop row of training data:\\n'\n",
    "\n",
    "print train_data [0,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Performing PCA on binarized Year, Month, Hour, DayOfWeek, District and XY coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components:   5     Variance Explained:  0.019597141065\n",
      "Components:  10     Variance Explained:  0.0346923201475\n",
      "Components:  15     Variance Explained:  0.0478148962346\n",
      "Components:  20     Variance Explained:  0.0558327942307\n",
      "Components:  25     Variance Explained:  0.0620686855089\n",
      "Components:  30     Variance Explained:  0.067480872356\n",
      "Components:  35     Variance Explained:  0.072116252412\n",
      "Components:  40     Variance Explained:  0.0757860630109\n",
      "Components:  45     Variance Explained:  0.0785184129746\n",
      "Components:  50     Variance Explained:  0.0803437074549\n",
      "Components:  55     Variance Explained:  0.0818741339834\n",
      "Components:  60     Variance Explained:  0.0830849656831\n",
      "Components:  65     Variance Explained:  0.084168500895\n",
      "Components:  70     Variance Explained:  0.0850772499066\n",
      "Components:  75     Variance Explained:  0.0858472412636\n",
      "Components:  80     Variance Explained:  0.0864942576853\n",
      "Components:  85     Variance Explained:  0.0870557806639\n",
      "Components:  90     Variance Explained:  0.0875663376404\n",
      "Components:  95     Variance Explained:  0.0880240683415\n",
      "Components:  100     Variance Explained:  0.0884363237165\n",
      "Components:  105     Variance Explained:  0.0888063362215\n",
      "Components:  110     Variance Explained:  0.0891412272875\n",
      "Components:  115     Variance Explained:  0.0894276228541\n"
     ]
    }
   ],
   "source": [
    "pca_mod = PCA(n_components=120).fit(train_all[:200000,12:])\n",
    "for i in range(5,120, 5):\n",
    "    var = sum(pca_mod.explained_variance_ratio_[range(1,i)])\n",
    "    print 'Components: ', '{:2}'.format(i), '    Variance Explained: ', var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "We define subsets of the training and test data to use for development. Larger datasets will be used for full validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train labels: ', 39)\n",
      "('Test  labels: ', 39)\n"
     ]
    }
   ],
   "source": [
    "# Columns to use in baseline classification\n",
    "cols = range(13, 206)\n",
    "\n",
    "dev_data = train_data#[0:100000,]\n",
    "dev_labels = train_labels#[0:70000]\n",
    "t\n",
    "dev_test_data = test_data#[0:30000,]\n",
    "dev_test_labels = test_labels#[0:30000]\n",
    "\n",
    "print('Train labels: ', len(np.unique(dev_labels)))\n",
    "print('Test  labels: ', len(np.unique(dev_test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "We tested logistic regression as a simple, baseline model since it is intuitive and the prediction factors are easily identifiable. First we use a gridsearch on C values to find optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring='neg_log_loss', verbose=0)\n",
      "{'C': 0.3}\n"
     ]
    }
   ],
   "source": [
    "c_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "params = {'C': c_values}\n",
    "grid = GridSearchCV(estimator=LogisticRegression(), param_grid=params, scoring='neg_log_loss')\n",
    "grid.fit(dev_data[:,cols], dev_labels)\n",
    "print grid\n",
    "print grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training\n",
      "Completed training\n",
      "Begin prediction\n",
      "Completed prediction\n",
      "\n",
      "Log score:  2.50419479283 \n",
      "\n",
      "Probability prediction examples:  [  2.05772035e-03   9.64082077e-02   1.41803703e-04   5.20219719e-04\n",
      "   2.23870262e-02   1.08380566e-02   3.00517150e-03   3.60632917e-02\n",
      "   6.72280000e-03   3.22072579e-04   2.36610824e-04   5.36850724e-03\n",
      "   3.06558696e-03   4.96320498e-03   1.78035746e-04   1.67127343e-03\n",
      "   1.13397203e-01   2.13864695e-03   7.14063657e-04   6.93625772e-02\n",
      "   7.59141540e-02   1.19626048e-01   6.56260835e-05   5.85852760e-02\n",
      "   1.38840981e-04   2.13658750e-02   1.54885118e-02   7.72431219e-03\n",
      "   5.38876447e-03   5.20608425e-04   2.63852127e-03   8.80772743e-04\n",
      "   2.31179088e-02   4.62691085e-05   1.03402113e-02   5.53862793e-02\n",
      "   1.67644117e-01   4.35304298e-02   1.20353940e-02]\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression(C=0.3)\n",
    "print 'Begin training'\n",
    "lgr.fit(dev_data[:,cols], dev_labels)\n",
    "print 'Completed training'\n",
    "\n",
    "print 'Begin prediction'\n",
    "pred_probs = lgr.predict_proba(dev_test_data[:,cols])\n",
    "\n",
    "print 'Completed prediction\\n'\n",
    "\n",
    "print 'Log score: ', metrics.log_loss(dev_test_labels, pred_probs, labels=range(0,39)), '\\n'\n",
    "\n",
    "print 'Probability prediction examples: ', pred_probs[0,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors\n",
    "The K nearest neighbors classifier uses clustering in order to group features together. \n",
    "It seems appropriate that we use this algorithm, considering that we are dealing with actual neighborhoods in the data. We need to first find the optimal number of clusters to use.\n",
    "Additionally, we will change the parameters to include the actual X/Y coordinate values instead of the combined string proxy version since it can use the numerical data to form actual clusters in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knnCols = range(7,13)\n",
    "\n",
    "n_values = [200, 300, 500]\n",
    "params = {'n_neighbors': n_values}\n",
    "grid = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=params, scoring='neg_log_loss')\n",
    "grid.fit(dev_data[:,knnCols], dev_labels)\n",
    "print grid\n",
    "print grid.get_params(0)\n",
    "print grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin fitting\n",
      "Completed fitting\n",
      "Begin prediction\n",
      "Completed prediction\n",
      "2.86875081942\n"
     ]
    }
   ],
   "source": [
    "knnCols = range(7,13)\n",
    "knn = KNeighborsClassifier(n_neighbors=500)\n",
    "print('Begin fitting')\n",
    "knn.fit(dev_data[:,knnCols], dev_labels)\n",
    "print('Completed fitting')\n",
    "print('Begin prediction')\n",
    "predicted = knn.predict_proba(dev_test_data[:,knnCols])\n",
    "print('Completed prediction')\n",
    "print metrics.log_loss(dev_test_labels, predicted, labels=range(0,39))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin fitting\n",
      "Completed fitting\n",
      "Begin prediction\n",
      "Completed prediction\n",
      "7.00851969519\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50)\n",
    "print('Begin fitting')\n",
    "rfc.fit(dev_data[:,knnCols],dev_labels)\n",
    "print('Completed fitting')\n",
    "print('Begin prediction')\n",
    "predicted = rfc.predict_proba(dev_test_data[:,knnCols])\n",
    "print('Completed prediction')\n",
    "print metrics.log_loss(dev_test_labels, predicted, labels=range(0,39))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "#### Bernoulli\n",
    "Bernoulli Naive Bayes is suited towards discrete data, specifically binary. \n",
    "The data for this problem is binary in a category vs all so it may be a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Perform BernoulliNB analysis here including parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial\n",
    "Multi-class options may be better fit for data than Binomial\n",
    "Use label encoded data as opposed to binary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Perform MultinomialNB analysis here including parameter optimizaiton of alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian\n",
    "Gaussian models for each of the classes? Does this even make sense, given that there are so many categories?\n",
    "Will models with too few skew the data incorrectly? Discuss this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Perform GaussianNB analysis here for all categories,\n",
    "### predict based on log odds for each of the individual category classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Perform SVM analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_final = dateAttributes(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting  Year\n",
      "Finished  Year\n",
      "Starting  Month\n",
      "Finished  Month\n",
      "Starting  Hour\n",
      "Finished  Hour\n",
      "Starting  DayOfWeek\n",
      "Finished  DayOfWeek\n",
      "Starting  PdDistrict\n",
      "Finished  PdDistrict\n"
     ]
    }
   ],
   "source": [
    "# Binarize columns in initial data processing above and add to dataframe\n",
    "for column in bin_cols:\n",
    "    print 'Starting ', column\n",
    "    dummies = pd.get_dummies(test_final[column])\n",
    "    test_final[dummies.columns] = dummies\n",
    "    print 'Finished ', column\n",
    "\n",
    "# Copy IDs fr each row in final test data just in case...\n",
    "ids = test_final['Id']\n",
    "\n",
    "# Convert final data data intl numpy object for processing\n",
    "test_final_data = np.array(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0\n",
      " 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0]\n"
     ]
    }
   ],
   "source": [
    "print test_final_data[0,range(12,len(test_final.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training\n",
      "Completed training\n"
     ]
    }
   ],
   "source": [
    "# Train final model on all training data available\n",
    "lgrFinal = LogisticRegression()\n",
    "\n",
    "print 'Beginning training'\n",
    "lgrFinal.fit(train_data_all[:,cols], labels_all)\n",
    "print 'Completed training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin prediction\n",
      "Completed training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-999a565bfb87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Completed training'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Output example:'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cat' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate predicted probabilities for each category for final test data\n",
    "\n",
    "print 'Begin prediction'\n",
    "pred_probs = lgrFinal.predict_proba(test_final_data[:,range(11,len(test_final.columns))])\n",
    "print 'Completed prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to file:  ../data/matthew_submission.csv\n",
      "File creation complete\n"
     ]
    }
   ],
   "source": [
    "# Output predictions to csv file for kaggle submission\n",
    "output = pd.DataFrame(pred_probs, columns=categories)\n",
    "file_name = '../data/matthew_submission.csv'\n",
    "print 'Output to file: ', file_name\n",
    "output.to_csv(file_name, index=True, index_label='Id')\n",
    "print 'File creation complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262, 39)\n"
     ]
    }
   ],
   "source": [
    "print output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23228\n"
     ]
    }
   ],
   "source": [
    "print len(np.unique(train_all['Address']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
